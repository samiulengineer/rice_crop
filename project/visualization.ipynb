{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import config\n",
    "import pathlib\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import *\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "from dataset import read_img\n",
    "from matplotlib import pyplot as plt\n",
    "import subprocess\n",
    "import pyperclip\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# train_df = pd.read_csv(config.train_dir)\n",
    "# test_df =  pd.read_csv(config.test_dir)\n",
    "# valid_df = pd.read_csv(config.valid_dir)\n",
    "# p_train_json = config.p_train_dir\n",
    "# p_test_json = config.p_test_dir\n",
    "# p_valid_json = config.p_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images = 430\n",
      "Total number of test images = 54\n",
      "Total number of validation images = 54\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of training images = {len(train_df)}\")\n",
    "print(f\"Total number of test images = {len(test_df)}\")\n",
    "print(f\"Total number of validation images = {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance_check(patchify, data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        checking class percentage in full dataset\n",
    "    Arguments:\n",
    "        patchify (bool): TRUE if want to check class balance for patchify experiments\n",
    "        data_dir (str): directory where data files are saved \n",
    "    Return:\n",
    "        class percentage\n",
    "    \"\"\"\n",
    "    if patchify:\n",
    "        with open(data_dir, \"r\") as j:\n",
    "            train_data = json.loads(j.read())\n",
    "        labels = train_data[\"masks\"]\n",
    "        patch_idx = train_data[\"patch_idx\"]\n",
    "\n",
    "    else:\n",
    "        train_data = pd.read_csv(data_dir)\n",
    "        labels = train_data.masks.values\n",
    "        patch_idx = None\n",
    "\n",
    "    total = 0\n",
    "    class_name = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        with rasterio.open(labels[i]) as msk:\n",
    "            mask = msk.read(1)\n",
    "        # mask = cv2.imread(labels[i])\n",
    "        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        # mask[mask==2]=0\n",
    "        if patchify:\n",
    "            idx = patch_idx[i]\n",
    "            mask = mask[idx[0] : idx[1], idx[2] : idx[3]]\n",
    "\n",
    "        total_pix = mask.shape[0] * mask.shape[1]\n",
    "        total += total_pix\n",
    "\n",
    "        dic = {}\n",
    "        keys = np.unique(mask)\n",
    "        for i in keys:\n",
    "            dic[i] = np.count_nonzero(mask == i)\n",
    "\n",
    "        for key, value in dic.items():\n",
    "            if key in class_name.keys():\n",
    "                class_name[key] = value + class_name[key]\n",
    "            else:\n",
    "                class_name[key] = value\n",
    "\n",
    "    for key, val in class_name.items():\n",
    "        class_name[key] = (val / total) * 100\n",
    "\n",
    "    print(\"Class percentage:\")\n",
    "    for key, val in class_name.items():\n",
    "        print(\"class pixel: {} = {}\".format(key, val))\n",
    "    print(f\"unique value in the mask {class_name.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class percentage of traning data before patch\n",
      "Class percentage:\n",
      "class pixel: 2.0 = 75.63027670217114\n",
      "class pixel: 1.0 = 24.369723297828852\n",
      "unique value in the mask dict_keys([2.0, 1.0])\n",
      ".........................................................................................\n",
      "class percentage of traning data after patch\n",
      "Class percentage:\n",
      "class pixel: 1.0 = 65.69205356629013\n",
      "class pixel: 2.0 = 34.30794643370988\n",
      "unique value in the mask dict_keys([1.0, 2.0])\n"
     ]
    }
   ],
   "source": [
    "print(\"class percentage of traning data before patch\")\n",
    "class_balance_check(patchify=False, data_dir=config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"class percentage of traning data after patch\")\n",
    "class_balance_check(patchify=True, data_dir=config.p_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_height_width(data_dir):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        check unique hight and width of images from dataset\n",
    "    Arguments:\n",
    "        data_dir (str): path to csv file\n",
    "    Return:\n",
    "        print all the unique height and width\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_dir)\n",
    "\n",
    "\n",
    "    print(\"Dataset:  \", data.shape)\n",
    "\n",
    "    input_img = data.feature_ids.values\n",
    "    input_mask = data.masks.values\n",
    "\n",
    "    input_img_shape = []\n",
    "    input_mask_shape = []\n",
    "\n",
    "    for i in range(len(input_img)):\n",
    "        with rasterio.open(input_img[i]) as im:\n",
    "            img = im.read()\n",
    "        with rasterio.open(input_mask[i]) as msk:\n",
    "            mask = msk.read()\n",
    "        # img = cv2.imread(input_img[i])\n",
    "        # mask = cv2.imread(input_mask[i])\n",
    "\n",
    "        if img.shape not in input_img_shape:\n",
    "            input_img_shape.append(img.shape)\n",
    "\n",
    "        if mask.shape not in input_mask_shape:\n",
    "            input_mask_shape.append(mask.shape)\n",
    "\n",
    "    print(\"Input image shapes: \", input_img_shape)\n",
    "    print(\"Input mask shapes: \", input_mask_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique height and width of training dataset\n",
      "Dataset:   (430, 2)\n",
      "Input image shapes:  [(3, 512, 512)]\n",
      "Input mask shapes:  [(1, 512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of testing dataset\n",
      "Dataset:   (54, 2)\n",
      "Input image shapes:  [(3, 512, 512)]\n",
      "Input mask shapes:  [(1, 512, 512)]\n",
      ".........................................................................................\n",
      "Unique height and width of validation dataset\n",
      "Dataset:   (54, 2)\n",
      "Input image shapes:  [(3, 512, 512)]\n",
      "Input mask shapes:  [(1, 512, 512)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique height and width of training dataset\")\n",
    "check_height_width(config.train_dir)\n",
    "print(\".........................................................................................\")\n",
    "print(\"Unique height and width of testing dataset\")\n",
    "check_height_width(config.test_dir)\n",
    "print(\".........................................................................................\")\n",
    "\n",
    "print(\"Unique height and width of validation dataset\")\n",
    "check_height_width(config.valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_from_path(csv_path=config.csv_logger_path):\n",
    "    csv_list = []\n",
    "    # Iterate through each subdirectory\n",
    "    for folder in csv_path.iterdir():\n",
    "        # Check if the entry is a directory\n",
    "        if folder.is_dir():\n",
    "            # Iterate through files in the subdirectory\n",
    "            for file in folder.iterdir():\n",
    "                # Check if the entry is a file\n",
    "                if file.is_file():\n",
    "                    csv_list.append(file)\n",
    "    # print(csv_list)\n",
    "    return csv_list\n",
    "                    \n",
    "\n",
    "def _plot_from_csv(csv_path, name, x_axis_name, y_axis_name, columns_to_plot=None):\n",
    "    pathlib.Path((config.root_dir /\"logs\" / \"plots\"/\"metrics_plots\")).mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    epochs = df['epoch']\n",
    "    if columns_to_plot is not None:\n",
    "        columns_to_plot = columns_to_plot\n",
    "    else:\n",
    "        columns_to_plot = df.columns.to_list()[1:]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(epochs, df[column], label=column, linewidth=3.0,\n",
    "            marker=\"o\",\n",
    "            markersize=5)\n",
    "\n",
    "    plt.title(f\"{y_axis_name}_over_{x_axis_name}\")\n",
    "    plt.xlabel(x_axis_name)\n",
    "    plt.ylabel(y_axis_name)\n",
    "    plt.xticks(epochs.astype(int))\n",
    "    plt.legend()\n",
    "    plt.savefig(config.root_dir/\"logs\"/\"plots\"/\"metrics_plots\"/name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_vs_epochs(csv_path, name, x_axis_name= \"Epochs\", y_axis_name=\"Metrics_score\",columns_to_plot=None):\n",
    "    _plot_from_csv(csv_path=csv_path, name=name,x_axis_name=x_axis_name, y_axis_name=y_axis_name, columns_to_plot=columns_to_plot)\n",
    "\n",
    "def plot_metric_vs_epochs_vs_models(metric_name=\"val_f1_score\"):\n",
    "    pathlib.Path((config.root_dir /\"logs\"/ \"plots\"/\"csv_for_plotting\")).mkdir(parents=True, exist_ok=True)\n",
    "    csv_list = return_csv_from_path()\n",
    "    result_df = pd.DataFrame()\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        result_df[os.path.basename(csv_path)] = df[metric_name]\n",
    "    result_df.index.name = \"epoch\"\n",
    "    result_df.to_csv(os.path.join(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\"), encoding='utf-8',index=True, header=True)\n",
    "    _plot_from_csv(config.root_dir/\"logs\"/\"plots\"/\"csv_for_plotting\"/f\"{metric_name}_vs_epoch.csv\", x_axis_name= \"Epochs\", y_axis_name=metric_name, name=metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/hdd2/mdsamiul/project/imseg_rice/logs/csv_logger/unet/unet_ex_training_ep_20.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4031388/1112339532.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metrics_vs_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_logger_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"unet\"\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"unet_ex_training_ep_20.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_metrics_vs_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_logger_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"unet\"\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"unet_ex_training_ep_20.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_to_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_metric_vs_epochs_vs_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_metric_vs_epochs_vs_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4031388/1927682202.py\u001b[0m in \u001b[0;36mplot_metrics_vs_epochs\u001b[0;34m(csv_path, name, x_axis_name, y_axis_name, columns_to_plot)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_metrics_vs_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Metrics_score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns_to_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0m_plot_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_axis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_axis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_axis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_metric_vs_epochs_vs_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_f1_score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4031388/1927682202.py\u001b[0m in \u001b[0;36m_plot_from_csv\u001b[0;34m(csv_path, name, x_axis_name, y_axis_name, columns_to_plot)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_plot_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_axis_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m\"logs\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"plots\"\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"metrics_plots\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns_to_plot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/hdd2/mdsamiul/project/imseg_rice/logs/csv_logger/unet/unet_ex_training_ep_20.csv'"
     ]
    }
   ],
   "source": [
    "plot_metrics_vs_epochs(config.csv_logger_path/\"unet\"/\"unet_ex_training_ep_20.csv\",name='metrics')\n",
    "plot_metrics_vs_epochs(config.csv_logger_path/\"unet\"/\"unet_ex_training_ep_20.csv\",name='metrics',columns_to_plot=[\"f1_score\"])\n",
    "plot_metric_vs_epochs_vs_models()\n",
    "plot_metric_vs_epochs_vs_models(metric_name=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_clip(array,pct=[2.5,97.5]):\n",
    "    array_min, array_max = np.nanpercentile(array,pct[0]), np.nanpercentile(array,pct[1])\n",
    "    clip = (array - array_min) / (array_max - array_min)\n",
    "    clip[clip>1]=1\n",
    "    clip[clip<0]=0\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rio.open(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset/input/tile_0_0_vVvHdF_Area1_17-18_Beg.tif\") as src:\n",
    "#     with rio.open(\n",
    "#             'RGB_Temp.tif', 'w+',\n",
    "#             driver='GTiff',\n",
    "#             dtype= rio.float32,\n",
    "#             count=3,\n",
    "#             crs = src.crs,\n",
    "#             width=src.width,\n",
    "#             height=src.height,\n",
    "#             transform=src.transform,\n",
    "#         ) as dst:\n",
    "#         V = pct_clip(src.read(1))\n",
    "#         dst.write(V.astype(rio.float32),1)\n",
    "#         V = pct_clip(src.read(2))\n",
    "#         dst.write(V.astype(rio.float32),2)\n",
    "#         V = pct_clip(src.read(3))\n",
    "#         dst.write(V.astype(rio.float32),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rio.open(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset/input/tile_0_0_vVvHdF_Area1_17-18_Beg.tif\") as src:\n",
    "#     # img= np.zeros((3,512,512))\n",
    "#     img= pct_clip(src.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_colour_read(path):\n",
    "    img= np.zeros((3,512,512))\n",
    "    with rasterio.open(path) as src:\n",
    "        for i in range(3):\n",
    "            img[i,:,:]= pct_clip(src.read(i+1))\n",
    "            \n",
    "    return img, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_colour_read_bt(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        global h,w\n",
    "        h,w =src.shape\n",
    "        img= np.zeros((3,h,w))\n",
    "        # print(img.shape)\n",
    "        for i in range(3):\n",
    "            img[i,:,:]= pct_clip(src.read(i+1))\n",
    "            \n",
    "    return img, src\n",
    "# print(\"displaying training images and masks\")\n",
    "# display_all(data=train_df,name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3379, 3286, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((h,w,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots()\n",
    "# with rio.open(\"RGB_Temp.tif\") as src2:\n",
    "#     show(src2.read(),transform=src2.transform,ax=ax)\n",
    "#     ax.grid(False)  # Turn off gridlines along the borders\n",
    "#     ax.set_xticks([])  # Remove x-axis ticks\n",
    "#     ax.set_yticks([])  # Remove y-axis ticks\n",
    "#     ax.set_xlabel('')  # Remove x-axis label\n",
    "#     ax.set_ylabel('')  # Remove y-axis label\n",
    "# plt.show()\n",
    "# plt.savefig(\"false_color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_dir = root_dir / \"data/dataset-nsr-1-bt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(data, name):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        save all images into single figure\n",
    "    Arguments:\n",
    "        data : data file holding images path\n",
    "        directory (str) : path to save images\n",
    "    Return:\n",
    "        save images figure into directory\n",
    "    \"\"\"\n",
    "\n",
    "    pathlib.Path((visualization_dir / \"display\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"test\")).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path((visualization_dir / \"display\"/\"valid\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        image,src = false_colour_read_bt(data.feature_ids.values[i])\n",
    "        mask = read_img(data.masks.values[i], label=True)\n",
    "        print(\"................................\")\n",
    "        print(f\"image_shape: {image.shape}\")\n",
    "        print(f\"mask_shape: {mask.shape}\")\n",
    "        print(\"................................\")\n",
    "        id = data.feature_ids.values[i].split(\"/\")[-1]\n",
    "        display_list = {\"image\": image, \"label\": mask}\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        title = list(display_list.keys())\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i + 1)\n",
    "            plt.title(title[i])\n",
    "            if title[i]=='image':\n",
    "                ax = plt.gca()\n",
    "                show(display_list[title[i]],transform=src.transform, ax=ax)\n",
    "            else:\n",
    "                plt.imshow((display_list[title[i]]), cmap=\"gray\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        prediction_name = \"img_id_{}.png\".format(id)  # create file name to save\n",
    "        plt.savefig(\n",
    "            os.path.join((visualization_dir / \"display\"/ name), prediction_name),\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=800,\n",
    "        )\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1-bt/data/csv/train.csv\")\n",
    "test_df =  pd.read_csv(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1-bt/data/csv/test.csv\")\n",
    "valid_df = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1-bt/data/csv/valid.csv\")\n",
    "# p_train_json = config.p_train_dir\n",
    "# p_test_json = config.p_test_dir\n",
    "# p_valid_json = config.p_valid_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(directory, in_channels=None, label=False, patch_idx=None, height=256, width=256):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        read image with rasterio and normalize the feature\n",
    "    Arguments:\n",
    "        directory (str): image path to read\n",
    "        in_channels (bool): number of channels to read\n",
    "        label (bool): TRUE if the given directory is mask directory otherwise False\n",
    "        patch_idx (list): patch indices to read\n",
    "    Return:\n",
    "        numpy.array\n",
    "    \"\"\"\n",
    "\n",
    "    # for musk images\n",
    "    if label:\n",
    "        with rasterio.open(directory) as fmask: # opening the directory\n",
    "            mask = fmask.read(1)    # read the image (Data from a raster band can be accessed by the band’s index number. Following the GDAL convention, bands are indexed from 1. [int or list, optional] – If indexes is a list, the result is a 3D array, but is a 2D array if it is a band index number.\n",
    "        \n",
    "        mask[mask == 2.0] = 0\n",
    "        mask[mask == 1.0] = 1\n",
    "        # np.swapaxes(mask,0,2)\n",
    "        # mask[mask == 255] = 1\n",
    "        # mask[mask == 170] = 2\n",
    "        # mask[mask == 85] = 2\n",
    "        mask = mask[... , np.newaxis]\n",
    "        mask = mask.astype(\"int32\")\n",
    "        # print(\".......mask...............\")\n",
    "        # print(mask.shape)\n",
    "    \n",
    "        if patch_idx:\n",
    "            # extract patch from original mask\n",
    "            return mask[patch_idx[0]:patch_idx[1], patch_idx[2]:patch_idx[3]]\n",
    "        else:\n",
    "            return mask #np.expand_dims(mask, axis=2)\n",
    "    # for features images\n",
    "    else:\n",
    "        # read N number of channels\n",
    "        with rasterio.open(directory) as inp:\n",
    "            X =inp.read()\n",
    "        X= np.swapaxes(X,0,2)\n",
    "        X = (X-mean)/std\n",
    "        if patch_idx:\n",
    "            # extract patch from original features\n",
    "            return X[patch_idx[0]:patch_idx[1], patch_idx[2]:patch_idx[3], :]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying training images and masks\n",
      "................................\n",
      "image_shape: (3, 3379, 3286)\n",
      "mask_shape: (3379, 3286, 1)\n",
      "................................\n",
      "................................\n",
      "image_shape: (3, 1962, 2769)\n",
      "mask_shape: (1962, 2769, 1)\n",
      "................................\n",
      "................................\n",
      "image_shape: (3, 3406, 3286)\n",
      "mask_shape: (3406, 3286, 1)\n",
      "................................\n",
      "................................\n",
      "image_shape: (3, 3379, 3286)\n",
      "mask_shape: (3379, 3286, 1)\n",
      "................................\n",
      "................................\n",
      "image_shape: (3, 1962, 2769)\n",
      "mask_shape: (1962, 2769, 1)\n",
      "................................\n",
      "................................\n",
      "image_shape: (3, 3406, 3286)\n",
      "mask_shape: (3406, 3286, 1)\n",
      "................................\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying training images and masks\")\n",
    "display_all(data=train_df,name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/hdd2/mdsamiul/project/imseg_rice/data/dataset-nsr-3-bt')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir / \"data/dataset-nsr-3-bt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying testing images and masks\n",
      "................................\n",
      "image_shape: (3, 3391, 3260)\n",
      "mask_shape: (3391, 3260, 1)\n",
      "................................\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying testing images and masks\")\n",
    "display_all(data=test_df, name = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "displaying validation images and masks\n",
      "................................\n",
      "image_shape: (3, 3391, 3260)\n",
      "mask_shape: (3391, 3260, 1)\n",
      "................................\n"
     ]
    }
   ],
   "source": [
    "print(\"displaying validation images and masks\")\n",
    "display_all(data=valid_df, name= \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean across 1st band: 0.21080770438931093\n",
      "Standard deviation across 1st band: 0.08680501088155813\n",
      "Average mean across 2bd band: 0.16065762694462385\n",
      "Standard deviation across 2nd band: 0.07915457728508787\n",
      "Average mean across 3rd band: 0.07456150900220794\n",
      "Standard deviation across 3rd band: 0.037746545995537006\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1/data/csv/train.csv\")\n",
    "features_path = train_df[\"feature_ids\"].to_list()\n",
    "def calculate_stats(file_paths):\n",
    "    all_data1 = []\n",
    "    all_data2 = []\n",
    "    all_data3 = []\n",
    "    for file_path in file_paths:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data1 = src.read(1)  # Read the first band\n",
    "            all_data1.append(data1)\n",
    "            data2 = src.read(2)  # Read the first band\n",
    "            all_data2.append(data2)\n",
    "            data3 = src.read(3)  # Read the first band\n",
    "            all_data3.append(data3)\n",
    "\n",
    "    # Stack all the data into a single numpy array\n",
    "    stacked_data1 = np.stack(all_data1)\n",
    "    stacked_data2 = np.stack(all_data2)\n",
    "    stacked_data3 = np.stack(all_data3)\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean1 = np.mean(stacked_data1)\n",
    "    std_dev1 = np.std(stacked_data1)\n",
    "    mean2 = np.mean(stacked_data2)\n",
    "    std_dev2 = np.std(stacked_data2)\n",
    "    mean3 = np.mean(stacked_data3)\n",
    "    std_dev3 = np.std(stacked_data3)\n",
    "    print(\"Average mean across 1st band:\", mean1)\n",
    "    print(\"Standard deviation across 1st band:\", std_dev1)\n",
    "    print(\"Average mean across 2bd band:\", mean2)\n",
    "    print(\"Standard deviation across 2nd band:\", std_dev2)\n",
    "    print(\"Average mean across 3rd band:\", mean3)\n",
    "    print(\"Standard deviation across 3rd band:\", std_dev3)\n",
    "\n",
    "# Example list of file paths\n",
    "calculate_stats(features_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean across all files: 0.14867561344538044\n",
      "Standard deviation across all files: 0.0907785301413935\n"
     ]
    }
   ],
   "source": [
    "# /mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-3/data/csv\n",
    "train_df = pd.read_csv(\"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1/data/csv/train.csv\")\n",
    "features_path = train_df[\"feature_ids\"].to_list()\n",
    "def calculate_stats(file_paths):\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = src.read()  \n",
    "            # print(data.shape)\n",
    "            all_data.append(data)\n",
    "\n",
    "    # Stack all the data into a single numpy array\n",
    "    stacked_data = np.stack(all_data)\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = np.mean(stacked_data)\n",
    "    std_dev = np.std(stacked_data)\n",
    "\n",
    "    return mean, std_dev\n",
    "\n",
    "# Example list of file paths\n",
    "\n",
    "mean, std_dev = calculate_stats(features_path)\n",
    "\n",
    "print(\"Average mean across all files:\", mean)\n",
    "print(\"Standard deviation across all files:\", std_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"/mnt/hdd2/mdsamiul/project/dataset/trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(path, out_path, tiles_size=2048, stride=1024):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each file in the path\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Get metadata and calculate number of tiles in each dimension\n",
    "            meta = src.meta\n",
    "            meta[\"height\"]= tiles_size\n",
    "            meta[\"width\"]= tiles_size\n",
    "            # print(meta)\n",
    "            height, width = src.shape\n",
    "            num_rows = math.ceil((height - tiles_size) / stride + 1)\n",
    "            num_cols = math.ceil((width - tiles_size) / stride + 1)\n",
    "            total_tiles = num_rows* num_cols\n",
    "            print(f\"shape of the image before tiles : {src.shape}\")\n",
    "            print(f\"number of tiles={total_tiles}\")\n",
    "            print(\"..................................................\")\n",
    "            # Iterate over each tile\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    # Calculate window coordinates\n",
    "                    row_start = row * stride\n",
    "                    row_stop = min(row_start + tiles_size, height)\n",
    "                    col_start = col * stride\n",
    "                    col_stop = min(col_start + tiles_size, width)\n",
    "                    \n",
    "                    # Read the tile data\n",
    "                    # window = Window(x0, y0, x1 - x0, y1 - y0)\n",
    "                    window = Window.from_slices((row_stop-stride, row_stop), (col_stop-stride, col_stop))\n",
    "                    tile_data = src.read(window=window)\n",
    "                    # print(\"...........\")\n",
    "                    # print(tile_data.shape)\n",
    "                    # Save the tile with a suffix of tile id\n",
    "                    # out_filename = f\"{os.path.splitext(filename)[0]}_tile_{row}_{col}.tif\"\n",
    "                    out_filename = f\"tile_{row}_{col}_{os.path.splitext(filename)[0]}.tif\"\n",
    "                    out_file_path = os.path.join(out_path, out_filename)\n",
    "                    with rasterio.open(out_file_path, 'w', **meta) as dst:\n",
    "                        dst.write(tile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/mnt/hdd2/mdsamiul/project/rice_crop_segmentation/data/dataset-nsr-1-bt/input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the image before tiles : (3391, 3260)\n",
      "number of tiles=9\n",
      "..................................................\n",
      "shape of the image before tiles : (3406, 3286)\n",
      "number of tiles=9\n",
      "..................................................\n",
      "shape of the image before tiles : (3379, 3286)\n",
      "number of tiles=9\n",
      "..................................................\n",
      "shape of the image before tiles : (1962, 2769)\n",
      "number of tiles=2\n",
      "..................................................\n",
      "shape of the image before tiles : (3406, 3286)\n",
      "number of tiles=9\n",
      "..................................................\n",
      "shape of the image before tiles : (3391, 3260)\n",
      "number of tiles=9\n",
      "..................................................\n",
      "shape of the image before tiles : (1962, 2769)\n",
      "number of tiles=2\n",
      "..................................................\n",
      "shape of the image before tiles : (3379, 3286)\n",
      "number of tiles=9\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "save_tiles(data,root_dir/\"data/dataset-nsr-1/input/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(datapath):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(datapath)\n",
    "    \n",
    "    for filename in files:\n",
    "        # Extract the file extension\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # Check if the filename starts with DEM_ab.tif\n",
    "        if filename.startswith(\"DEM_\"):\n",
    "            new_filename = filename.replace(\"DEM_\", \"\").replace(\".tif\", \"_nasadem.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VV_ab.tif\n",
    "        elif filename.startswith(\"VV_\"):\n",
    "            new_filename = filename.replace(\"VV_\", \"\").replace(\".tif\", \"_vv.tif\")\n",
    "        \n",
    "        # Check if the filename starts with VH_ab.tif\n",
    "        elif filename.startswith(\"VH_\"):\n",
    "            new_filename = filename.replace(\"VH_\", \"\").replace(\".tif\", \"_vh.tif\")\n",
    "        \n",
    "        # Check if the filename starts with GT_ab.tif\n",
    "        elif filename.startswith(\"GT_\"):\n",
    "            new_filename = filename.replace(\"GT_\", \"\")\n",
    "        \n",
    "        else:\n",
    "            # If none of the conditions are met, skip this file\n",
    "            raise ValueError(\"files_name_mismatch\")\n",
    "        \n",
    "        # Construct the new filepath\n",
    "        new_filepath = os.path.join(datapath, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(os.path.join(datapath, filename), new_filepath)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = config.dataset_dir\n",
    "rename_files(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal output saved to data_statistics.rtf\n"
     ]
    }
   ],
   "source": [
    "# Run the command in the terminal\n",
    "command = \"python visualization.py\"\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Get the terminal output\n",
    "terminal_output = result.stdout\n",
    "\n",
    "# Save the output to an RTF file\n",
    "rtf_filename = \"data_statistics.rtf\"\n",
    "with open(rtf_filename, \"w\") as rtf_file:\n",
    "    # rtf_file.write(\"{\\\\rtf1\\\\ansi\\n\")\n",
    "    rtf_file.write(terminal_output)\n",
    "    # rtf_file.write(\"}\")\n",
    "\n",
    "print(f\"Terminal output saved to {rtf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
